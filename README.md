<!-- -------------------------------------------------- -->
<!-- ðŸ©º HEALTHMATE | AI MEDICAL CHATBOT 2025 -->
<!-- -------------------------------------------------- -->

<p align="center">
  <img src="https://img.shields.io/badge/Project-HealthMate-blue?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Backend-FastAPI_+_Express-success?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Frontend-React_(MERN)-green?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/AI-Groq_+_LangChain-orange?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Voice-ElevenLabs_+_Whisper-yellow?style=for-the-badge"/>
</p>

---

# ðŸš€ HealthMate | AI Medical Chatbot 2025
*A multimodal AI system for real-time symptom analysis, doctor-style conversation, and intelligent diagnosis assistance.*

---

## ðŸŒŸ Introduction

**HealthMate** is a voice- and image-driven **AI medical chatbot** built to make healthcare guidance **accessible, empathetic, and conversational**.  
It enables users to **speak naturally**, **upload medical images**, and receive **intelligent health insights** generated by advanced AI models in real time.

The project integrates **speech, vision, and language** understanding to simulate realistic doctor-patient interactions, bridging the gap between everyday users and accessible healthcare technology.

---

## ðŸ§¾ Executive Summary

HealthMate combines **state-of-the-art AI technologies** across multiple domains:

- ðŸ§  **Language Understanding:** Powered by **Groq LLMs** and **LangChain RAG pipelines**  
- ðŸ—£ï¸ **Speech Recognition:** Implemented with **Whisper v3** for accurate transcription  
- ðŸ”Š **Voice Synthesis:** Natural doctor-like responses using **ElevenLabs Voice AI**  
- ðŸ©» **Image Analysis:** Symptom and wound detection via **CLIP + Vision Transformer**  
- âš™ï¸ **Backend Stack:** Hybrid **FastAPI + Express.js** architecture  
- ðŸ’» **Frontend Stack:** Built with **React (Vite)** and **Clerk authentication**

ðŸ©º HealthMate allows **two-way conversation** â€” the user speaks, AI listens and responds in **voice + text**, analyzing both **speech and uploaded images** for symptom detection.

---

## ðŸ“Š Key Features

| Feature | Description |
|:--------|:-------------|
| ðŸ—£ï¸ **Voice-to-Text Interface** | Record and transcribe speech using Whisper |
| ðŸ©» **Image Diagnosis** | Upload symptom images for AI-based detection |
| ðŸ’¬ **Conversational Memory** | Maintains ongoing context via conversation ID |
| ðŸ”‰ **Voice Responses** | AI speaks back with ElevenLabs voice synthesis |
| ðŸ” **User Authentication** | Managed securely via Clerk |
| âš¡ **Fast Inference** | Low latency powered by Groq LPU servers |

---

## âš™ï¸ Architecture Overview

### ðŸ§  Core Workflow

```mermaid
graph TD
    A["User Voice/Text Input"] --> B["Whisper STT"]
    B --> C["Groq LLM + LangChain RAG"]
    C --> D["Diagnosis + Recommendations"]
    D --> E["FastAPI/Express Middleware"]
    E --> F["Frontend UI (React)"]
    F --> G["Text & Voice Output"]
    F --> H["Doctor Response Visualization"]
    I["Image Upload"] --> J["CLIP/ViT Analysis"]
    J --> D


