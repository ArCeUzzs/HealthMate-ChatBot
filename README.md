<!-- -------------------------------------------------- -->
<!-- 🩺 HEALTHMATE | AI MEDICAL CHATBOT 2025 -->
<!-- -------------------------------------------------- -->

<p align="center">
  <img src="https://img.shields.io/badge/Project-HealthMate-blue?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Backend-FastAPI_+_Express-success?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Frontend-React_(MERN)-green?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/AI-Groq_+_LangChain_+_LLaMA-orange?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Voice-ElevenLabs_+_Whisper-yellow?style=for-the-badge"/>
</p>

---

# 🚀 HealthMate | AI Medical Chatbot 2025
*A multimodal AI system for real-time symptom analysis, doctor-style conversation, and intelligent diagnosis assistance.*

---

## 🎥 Demo Video

<p align="center">
  <a href="https://youtu.be/r0YPsOrYC4s" target="_blank">
    <img src="https://img.youtube.com/vi/r0YPsOrYC4s/0.jpg" alt="HealthMate Demo Video" width="70%" style="border-radius:10px;">
  </a>
</p>

> 🎬 *Watch the full demo on YouTube to see HealthMate in action!*

---

## 🌟 Introduction

**HealthMate** is a **voice- and image-driven AI medical chatbot** built to make healthcare guidance **accessible, empathetic, and conversational**.  
It allows users to **speak naturally**, **upload medical images**, and receive **real-time health insights** generated by advanced AI models.

The system integrates **speech, language, and visual understanding** to simulate realistic doctor–patient interactions — bridging the gap between everyday users and accessible healthcare technology.

---

## 🧾 Executive Summary

HealthMate combines **state-of-the-art AI technologies** across multiple domains:

- 🧠 **Language & Vision Reasoning:** Powered by **Meta LLaMA-4 Scout-17B (via Groq LPU)** for text + image diagnosis  
- 🗣️ **Speech Recognition:** **Whisper-Large-v3** for highly accurate transcription  
- 🔊 **Voice Synthesis:** **gTTS / ElevenLabs Voice AI** for natural doctor-like responses  
- 📚 **Context Retrieval:** **LangChain RAG** pipeline for evidence-based recommendations  
- ⚙️ **Backend Stack:** Hybrid **FastAPI + Express.js** architecture  
- 💻 **Frontend Stack:** **React (Vite)** + **Clerk authentication**

🩺 HealthMate supports **two-way doctor-like conversations** — the user speaks, uploads an image if needed, and receives **both spoken and written AI feedback**, all powered by Groq LPUs for ultra-low latency.

---

## 📊 Key Features

| Feature | Description |
|:--------|:-------------|
| 🗣️ **Voice Input (STT)** | Transcribe speech using Whisper-Large-v3 |
| 🩻 **Image + Text Diagnosis** | Combined reasoning with **LLaMA-4 Scout-17B** for text + image queries |
| 💬 **Conversational Memory** | Maintains session context via conversation ID |
| 🔉 **Voice Response (TTS)** | Converts AI output to voice using gTTS / ElevenLabs |
| 🔐 **User Authentication** | Securely managed via Clerk |
| ⚡ **Fast Inference** | Powered by **Groq LPU** for sub-second latency |

---

## ⚙️ Architecture Overview

## 🧠 System Architecture (Mermaid Diagram)

```mermaid
graph LR
    subgraph Frontend
        A1[React (Vite)]
        A2[Clerk Auth]
        A3[Voice Recorder]
        A4[Image Upload]
    end

    subgraph Backend_API
        B1[FastAPI]
        B2[Express.js]
        B3[Socket.io]
        B4[MongoDB]
    end

    subgraph AI_Services
        C1[Groq LLM]
        C2[LangChain RAG]
        C3[Whisper STT]
        C4[ElevenLabs TTS]
    end

    A1 --> B1
    A2 --> B1
    A3 --> B2
    A4 --> B2
    B1 --> C1
    B2 --> C2
    C1 --> C3
    C1 --> C4


### 🧠 Core Workflow

```mermaid
graph TD
    A["User Voice/Text Input"] --> B["Whisper STT"]
    B --> C["LangChain RAG Retriever"]
    C --> D["Groq-hosted LLaMA-4 Scout-17B"]
    D --> E["Diagnosis + Recommendations"]
    E --> F["FastAPI / Express Middleware"]
    F --> G["React Frontend UI"]
    G --> H["Voice Output (gTTS / ElevenLabs)"]
    I["Optional Image Upload"] --> D
