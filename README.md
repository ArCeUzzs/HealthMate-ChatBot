<!-- -------------------------------------------------- -->
<!-- 🩺 HEALTHMATE | AI MEDICAL CHATBOT 2025 -->
<!-- -------------------------------------------------- -->

<p align="center">
  <img src="https://img.shields.io/badge/Project-HealthMate-blue?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Backend-FastAPI_+_Express-success?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Frontend-React_(MERN)-green?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/AI-Groq_+_LangChain-orange?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Voice-ElevenLabs_+_Whisper-yellow?style=for-the-badge"/>
</p>

---

# 🚀 HealthMate | AI Medical Chatbot 2025
*A multimodal AI system for real-time symptom analysis, doctor-style conversation, and intelligent diagnosis assistance.*

---

## 🌟 Introduction

**HealthMate** is a voice- and image-driven **AI medical chatbot** built to make healthcare guidance **accessible, empathetic, and conversational**.  
It enables users to **speak naturally**, **upload medical images**, and receive **intelligent health insights** generated by advanced AI models in real time.

The project integrates **speech, vision, and language** understanding to simulate realistic doctor-patient interactions, bridging the gap between everyday users and accessible healthcare technology.

---

## 🧾 Executive Summary

HealthMate combines **state-of-the-art AI technologies** across multiple domains:

- 🧠 **Language Understanding:** Powered by **Groq LLMs** and **LangChain RAG pipelines**  
- 🗣️ **Speech Recognition:** Implemented with **Whisper v3** for accurate transcription  
- 🔊 **Voice Synthesis:** Natural doctor-like responses using **ElevenLabs Voice AI**  
- 🩻 **Image Analysis:** Symptom and wound detection via **CLIP + Vision Transformer**  
- ⚙️ **Backend Stack:** Hybrid **FastAPI + Express.js** architecture  
- 💻 **Frontend Stack:** Built with **React (Vite)** and **Clerk authentication**

🩺 HealthMate allows **two-way conversation** — the user speaks, AI listens and responds in **voice + text**, analyzing both **speech and uploaded images** for symptom detection.

---

## 📊 Key Features

| Feature | Description |
|:--------|:-------------|
| 🗣️ **Voice-to-Text Interface** | Record and transcribe speech using Whisper |
| 🩻 **Image Diagnosis** | Upload symptom images for AI-based detection |
| 💬 **Conversational Memory** | Maintains ongoing context via conversation ID |
| 🔉 **Voice Responses** | AI speaks back with ElevenLabs voice synthesis |
| 🔐 **User Authentication** | Managed securely via Clerk |
| ⚡ **Fast Inference** | Low latency powered by Groq LPU servers |

---

## ⚙️ Architecture Overview

### 🧠 Core Workflow

```mermaid
graph TD
    A["User Voice/Text Input"] --> B["Whisper STT"]
    B --> C["Groq LLM + LangChain RAG"]
    C --> D["Diagnosis + Recommendations"]
    D --> E["FastAPI/Express Middleware"]
    E --> F["Frontend UI (React)"]
    F --> G["Text & Voice Output"]
    F --> H["Doctor Response Visualization"]
    I["Image Upload"] --> J["CLIP/ViT Analysis"]
    J --> D


